{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7z8ZHDHl0fsf"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4997,
     "status": "ok",
     "timestamp": 1597054448668,
     "user": {
      "displayName": "김한수",
      "photoUrl": "",
      "userId": "13646096319260197263"
     },
     "user_tz": -540
    },
    "id": "Xj4hCSzOXzRR",
    "outputId": "7e887e8b-0261-4a48-f945-844137180d06"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas.util.testing as tm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vck2j9I3z5dA"
   },
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yXruowktz8xP"
   },
   "source": [
    "## Encoder & Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CO4bEIoHYbIX"
   },
   "outputs": [],
   "source": [
    "#Encoder List, Decoder List를 입력받아 Model을 구성하는 module\n",
    "class EncoderDecoder(nn.Module):\n",
    "    \"A standard Encoder-Decoder architecture. Base for this and many other models.\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    #src를 input으로 받아 encode를 수행한 뒤의 output을 return하는 method\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "    #encoder의 output(memory)를 input으로 받아 decode를 수행한 뒤의 output을 return하는 method\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "    \n",
    "    #encode와 decode를 수행한 결과를 return하는 method\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "me3f_A6p0mmB"
   },
   "source": [
    "## Generator (Linear + Softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ZZSE35xcCMT"
   },
   "outputs": [],
   "source": [
    "#Linear + softmax 수행하는 module\n",
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wPSH7bBm0otp"
   },
   "source": [
    "## clones (Copy the Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sS56pdnPcE5D"
   },
   "outputs": [],
   "source": [
    "#여러 module을 clone해 ModuleList로 return하는 method\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U-C4xZTJ0wY2"
   },
   "source": [
    "## Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UfNUYpsWC6El"
   },
   "source": [
    "[Layer Normalization 참고 논문](https://arxiv.org/abs/1607.06450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xW7SU97XgPYl"
   },
   "outputs": [],
   "source": [
    "#Normalization Layer Module\n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps  #epsilon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HJAMUy3Y00W8"
   },
   "source": [
    "## SublayerConnection (Residual Connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pToxPlzi2j9Y"
   },
   "outputs": [],
   "source": [
    "#Residual Connection Module\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"A residual connection followed by a layer norm.\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)             #Normalization\n",
    "        self.dropout = nn.Dropout(dropout)  #Dropout\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size\"\n",
    "        return x + self.dropout(sublayer(self.norm(x))) #layer(x) + x로 Residual Connection output return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ea3DbBd0tqv"
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cuiLmDIneNWY"
   },
   "outputs": [],
   "source": [
    "#Encoder Module\n",
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers.\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)  #Encoder Layer를 N개 clone해 저장\n",
    "        self.norm = LayerNorm(layer.size)   #Normalization Layer 추가\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:   #N개의 Encoder Layer들을 탐색\n",
    "            x = layer(x, mask)      #이전 Encoder Layer의 output을 다음 Encoder Layer의 input으로 사용\n",
    "        return self.norm(x)         #Mormalization 수행한 결과 return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6NcGAc3w1KDv"
   },
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cr7TF6eRi0Ho"
   },
   "outputs": [],
   "source": [
    "#Encoder Layer Module\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn                  #self attention layer\n",
    "        self.feed_forward = feed_forward    #feed forward layer\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)    #residual connection layer 2개\n",
    "        self.size = size\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        '''x를 input으로 self-attention layer에 넣은 output을\n",
    "        0번째 residual connection layer에 input으로 넣은 output을 x에 저장\n",
    "        attention의 Query, Key, Value를 모두 x로 사용'''\n",
    "        return self.sublayer[1](x, self.feed_forward)   #위의 결과를 input으로 feed forward layer에 넣은 output을 i1번째 residual connection layer에 input으로 넣은 output을 return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9eBbmkKr1MLS"
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vFQovdgh34I-"
   },
   "outputs": [],
   "source": [
    "#Decoder Module\n",
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with  masking\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)            #Decoder Layer를 N개 clone해 저장\n",
    "        self.norm = LayerNorm(layer.size)   #Normalization Layer\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask): #memory: Encoder의 최종 output\n",
    "        for layer in self.layers:                       #N개의 Decoder Layer를 탐색\n",
    "            x = layer(x, memory, src_mask, tgt_mask)    #이전 Layer의 output을 이후 Layer의 input으로 사용\n",
    "        return self.norm(x)                             #Normalization한 뒤의 output을 return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VfVlRwJp1Nkl"
   },
   "source": [
    "### Decoder Layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uW3ssEc24PZu"
   },
   "outputs": [],
   "source": [
    "#Decoder Layer Module\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed-forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn                  #masked self attention layer\n",
    "        self.src_attn = src_attn                    #self attention layer\n",
    "        self.feed_forward = feed_forward    #feed forward layer\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)    #residual connection layer 3개\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        '''#Output Embeding(tgt)을 input으로 넣은 masked self attention layer의 output을\n",
    "        0번째 residual connection layer에 input으로 넣은 output을 x에 저장\n",
    "        attention의 Query, Key, Value를 모두 x로 사용'''\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        '''위의 결과를 input으로 넣은 self attention layer의 output을\n",
    "        1번째 residual connection layer에 input으로 넣은 output을 x에 저장\n",
    "        attention의 Query, Key, Value를 x, m, m으로 사용'''\n",
    "        return self.sublayer[2](x, self.feed_forward) #위의 결과를 input으로 넣은 feed forward layer의 output을 2번째 residual connection layer에 input으로 넣은 output을 return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9d7d-AQP1TVw"
   },
   "source": [
    "## subsequent mask (masking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "umR6Lv3B49_Y"
   },
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZYNGgl2I0PKd"
   },
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hhBg8O_yBnCS"
   },
   "outputs": [],
   "source": [
    "#Attention 계산 method\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute Scaled Dot Product Attention\"\n",
    "    d_k = query.size(-1)    #d_k를 query의 size로 구함\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)    #QK^T / root(d_k) 계산\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)        #masking\n",
    "    p_attn = F.softmax(scores, dim = -1)                            #softmax 수행\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn                  #V를 곱한 값, V를 곱하지 않은 값 return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "thhRgfwX1iy_"
   },
   "source": [
    "## MultiHeadedAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0fIsxUQqI4CC"
   },
   "outputs": [],
   "source": [
    "#Multi-Head Attention Layer Module\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0         #d_model과 h가 나누어 떨어져야만 함 (d_k == d_v == d_model/h == 64이어야 하기 때문)\n",
    "        #We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h             #d_k 구하기\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)       #d_model * d_model size의 linear layer 4개 clone\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            #Same mask applied to all h heads\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)            #query size로 batch 횟수 구하기\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        #Query, Key, Value들을 linear에 넣은 뒤 모두 d_model 차원 vector에서 h x d_k 차원의 vector로 변경\n",
    "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch\n",
    "        #attention의 결과(V와 dot Product 하지 않은 값)을 x에 저장, V와 dot Product한 값을 self.attn에 저장\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) Concat using a view and apply a final linear\n",
    "        #h x d_k 차원으로 변환\n",
    "        x = x.transpose(1,2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)  #최종 linear layer의 output return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ftms8BFN1lJo"
   },
   "source": [
    "## PositionwiseFeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTPCQbGe3rk_"
   },
   "outputs": [],
   "source": [
    "#FeedForward Layer Module\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)         #d_model x d_ff Linear Layer\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)         #d_ff x d_model Linear Layer\n",
    "        self.dropout = nn.Dropout(dropout)          #dropout\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nGE4v1pK1pHM"
   },
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FSno6xZv4KbX"
   },
   "outputs": [],
   "source": [
    "#Embedding Module\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model) #d_model 차원으로 vocab Embedding\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)    #Embedding에 root(d_model)을 곱한 뒤 return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dvk4T0qb0Z35"
   },
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WbcnIOmXNCCV"
   },
   "source": [
    "[Positional Encoding 참고 논문](https://arxiv.org/pdf/1705.03122.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O9Skrg-25RlE"
   },
   "outputs": [],
   "source": [
    "#Positional Encoding Module\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "        #Compute the positional encodings once in log space\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rb9aa0M61unU"
   },
   "source": [
    "## make_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SWNlQMiT6eM5"
   },
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    c = copy.deepcopy   #copy.deepcopy 단축해 사용\n",
    "    attn = MultiHeadedAttention(h, d_model) #h x d_model size의 MultiHeadedAttention Layer 생성\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)    #d_model x d_ff size의 FeedForward Layer 생성\n",
    "    position = PositionalEncoding(d_model, dropout)     #d_model 차원의 PositionalEncoding 생성\n",
    "    model = EncoderDecoder(Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),  #Encoder\n",
    "                  Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),                  #Decoder\n",
    "                   nn.Sequential(Embeddings(d_model, src_vocab), c(position)),                          #src_embedding\n",
    "                   nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),                           #tgt_embedding\n",
    "                   Generator(d_model, tgt_vocab))                                                                          #Generator\n",
    "    #This was important from their code\n",
    "    #Initializer parameters with Glorot / fan_avg\n",
    "    for p in model.parameters():        #parameter 초기화\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)       #xavier 사용\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1929,
     "status": "ok",
     "timestamp": 1597054552289,
     "user": {
      "displayName": "김한수",
      "photoUrl": "",
      "userId": "13646096319260197263"
     },
     "user_tz": -540
    },
    "id": "yhHSDcug7VT6",
    "outputId": "c371aa78-eb91-49eb-b3d1-f78924f5dcd0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "#Small Example\n",
    "tmp_model = make_model(10, 10, 2)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "APxZF2BcznVL"
   },
   "source": [
    "#Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CIedrnTA5bNm"
   },
   "source": [
    "## Batches and Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hzJaFOmuzbWU"
   },
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training\"\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7EdcYRneA6Qi"
   },
   "source": [
    "##Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N2nm_CmDA8HV"
   },
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute):\n",
    "    \"Standard Training and Logging Function\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" % (i, loss / batch.ntokens, tokens / elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BWVxDi9IB-GM"
   },
   "source": [
    "## Training Data and Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-tUyNQd-B_r0"
   },
   "outputs": [],
   "source": [
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding\"\n",
    "    global max_src_in_batch, amx_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch, len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch, len(new.tgt) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xmjIzzIUFGHi"
   },
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ESX3XX8FHqS"
   },
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "\n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "            self._rate = rate\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def rate(self, step = None):\n",
    "        \"Implement 'lrate' above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * (self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "    \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000, torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QYFrTZ4qHuU8"
   },
   "source": [
    "## Regularization - Label Smooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XRCuOWMaHxO3"
   },
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim()  > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzMlZKp_JCqJ"
   },
   "source": [
    "#First Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tTNeXwjPJOO9"
   },
   "source": [
    "## Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMtIb-iMJM2p"
   },
   "outputs": [],
   "source": [
    "def data_gen(V, batch, nbatches):\n",
    "    \"Generate random data for a src-tgt copy task\"\n",
    "    for i in range(nbatches):\n",
    "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
    "        data[:, 0] = 1\n",
    "        src = Variable(data, requires_grad=False)\n",
    "        tgt = Variable(data, requires_grad=False)\n",
    "        yield Batch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Jk6MIBqJja7"
   },
   "source": [
    "## Loss Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "16_k4rJnJkvi"
   },
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train fuction\"\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)) / norm\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        #return loss.data[0] * norm\n",
    "        return loss.data * norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "abrb58BEKSfk"
   },
   "source": [
    "## Greedy Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 116651,
     "status": "ok",
     "timestamp": 1597055829969,
     "user": {
      "displayName": "김한수",
      "photoUrl": "",
      "userId": "13646096319260197263"
     },
     "user_tz": -540
    },
    "id": "siTBp5IMKUkE",
    "outputId": "9b953662-2c1f-48b6-c91a-10168960aded"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step: 1 Loss: 3.112231 Tokens per Sec: 509.232391\n",
      "Epoch Step: 1 Loss: 1.853409 Tokens per Sec: 734.600098\n",
      "tensor(1.8656)\n",
      "Epoch Step: 1 Loss: 1.922792 Tokens per Sec: 547.736450\n",
      "Epoch Step: 1 Loss: 1.673679 Tokens per Sec: 713.664734\n",
      "tensor(1.6339)\n",
      "Epoch Step: 1 Loss: 2.093796 Tokens per Sec: 556.614807\n",
      "Epoch Step: 1 Loss: 1.632262 Tokens per Sec: 717.539246\n",
      "tensor(1.5842)\n",
      "Epoch Step: 1 Loss: 1.717344 Tokens per Sec: 559.930542\n",
      "Epoch Step: 1 Loss: 1.189467 Tokens per Sec: 706.120361\n",
      "tensor(1.1804)\n",
      "Epoch Step: 1 Loss: 1.335050 Tokens per Sec: 554.291992\n",
      "Epoch Step: 1 Loss: 0.863616 Tokens per Sec: 726.482300\n",
      "tensor(0.9015)\n",
      "Epoch Step: 1 Loss: 1.164969 Tokens per Sec: 560.722229\n",
      "Epoch Step: 1 Loss: 0.795384 Tokens per Sec: 700.807861\n",
      "tensor(0.8346)\n",
      "Epoch Step: 1 Loss: 1.120978 Tokens per Sec: 553.277039\n",
      "Epoch Step: 1 Loss: 0.404769 Tokens per Sec: 712.831848\n",
      "tensor(0.4065)\n",
      "Epoch Step: 1 Loss: 0.817146 Tokens per Sec: 560.012634\n",
      "Epoch Step: 1 Loss: 0.434056 Tokens per Sec: 722.668518\n",
      "tensor(0.3681)\n",
      "Epoch Step: 1 Loss: 0.462003 Tokens per Sec: 564.629761\n",
      "Epoch Step: 1 Loss: 0.306886 Tokens per Sec: 719.928345\n",
      "tensor(0.2545)\n",
      "Epoch Step: 1 Loss: 0.291293 Tokens per Sec: 562.989197\n",
      "Epoch Step: 1 Loss: 0.248837 Tokens per Sec: 715.627625\n",
      "tensor(0.2970)\n"
     ]
    }
   ],
   "source": [
    "#Train the simple copy task\n",
    "V = 11\n",
    "criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "model = make_model(V, V, N=2)\n",
    "model_opt = NoamOpt(model.src_embed[0].d_model, 1, 400, torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    run_epoch(data_gen(V, 30, 20), model, SimpleLossCompute(model.generator, criterion, model_opt))\n",
    "    model.eval()\n",
    "    print(run_epoch(data_gen(V, 30, 5), model, SimpleLossCompute(model.generator, criterion, None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 853,
     "status": "ok",
     "timestamp": 1597055844196,
     "user": {
      "displayName": "김한수",
      "photoUrl": "",
      "userId": "13646096319260197263"
     },
     "user_tz": -540
    },
    "id": "im49OCQxL0jK",
    "outputId": "cb72a9c1-7e21-4201-c970-1141aea4ee18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  4,  5,  6,  5,  7,  8,  9, 10]])\n"
     ]
    }
   ],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(memory, src_mask, Variable(ys), Variable(subsequent_mask(ys.size(1)).type_as(src.data)))\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys\n",
    "\n",
    "model.eval()\n",
    "src = Variable(torch.LongTensor([[1,2,3,4,5,6,7,8,9,10]]))\n",
    "src_mask = Variable(torch.ones(1, 1, 10))\n",
    "print(greedy_decode(model, src, src_mask, max_len=10, start_symbol=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9520,
     "status": "ok",
     "timestamp": 1597054882821,
     "user": {
      "displayName": "김한수",
      "photoUrl": "",
      "userId": "13646096319260197263"
     },
     "user_tz": -540
    },
    "id": "5mhrngmrxUvf",
    "outputId": "a2f11413-e294-4fe5-8a19-02c5059c404d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchviz in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.6.0+cu101)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.18.5)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (0.10.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchviz\n",
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4547,
     "status": "ok",
     "timestamp": 1597055372877,
     "user": {
      "displayName": "김한수",
      "photoUrl": "",
      "userId": "13646096319260197263"
     },
     "user_tz": -540
    },
    "id": "bXhOlnqgxZgF",
    "outputId": "47fa4e7d-ec59-4bc2-872a-fb6ebd87befa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "from torch.autograd import Variable\n",
    "\n",
    "V = 11\n",
    "model = make_model(V, V, N=2)\n",
    "\n",
    "x = data_gen(V, 30, 20)\n",
    "for i, batch in enumerate(x):\n",
    "    make_dot(model(batch.src, batch.trg, batch.src_mask, batch.trg_mask), params=dict(model.named_parameters())).render(\"graph\", format=\"png\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 117
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 53805,
     "status": "ok",
     "timestamp": 1597054628302,
     "user": {
      "displayName": "김한수",
      "photoUrl": "",
      "userId": "13646096319260197263"
     },
     "user_tz": -540
    },
    "id": "lyVwXbAI0LFc",
    "outputId": "b8f9c06e-3e63-444d-ab6e-f7d6c2ad4a06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3455,
     "status": "ok",
     "timestamp": 1597054779557,
     "user": {
      "displayName": "김한수",
      "photoUrl": "",
      "userId": "13646096319260197263"
     },
     "user_tz": -540
    },
    "id": "NpqKEFAv1ueX",
    "outputId": "6afd58f8-4b5f-48c7-e4fa-480b5af3206f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive\n",
      "\u001b[0m\u001b[01;34m'Colab Notebooks'\u001b[0m/   \u001b[01;34mStandAloneDeepLearning\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%cd \"/content/gdrive/My Drive\"\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3813,
     "status": "ok",
     "timestamp": 1597054930530,
     "user": {
      "displayName": "김한수",
      "photoUrl": "",
      "userId": "13646096319260197263"
     },
     "user_tz": -540
    },
    "id": "gU0ksr590jZp",
    "outputId": "3255f6f6-bd63-40d4-ea0f-b0cb6e6a4230"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat '/content/graph.png': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!mv \"/content/graph.png\" \"/content/gdrive/My Drive/graph.png\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPO7EmVPiV5D4QAul0wbl6z",
   "collapsed_sections": [],
   "name": "transformer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
