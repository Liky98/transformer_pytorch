{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator, interleave_keys\n",
    "\n",
    "def itos(field, batch):  # batch에서 원본 sentence 얻는 함수\n",
    "    with torch.cuda.device_of(batch):\n",
    "        #batch = batch.T.tolist()\n",
    "        batch = batch.tolist()\n",
    "    batch = [[field.vocab.itos[ind] for ind in ex] for ex in batch]  # denumericalize\n",
    "    \n",
    "    def trim(s, t):  # 현재 token ~ <EOS> token 사이의 문장 return\n",
    "        sentence = []\n",
    "        for w in s:\n",
    "            if w == t:\n",
    "                break\n",
    "            sentence.append(w)\n",
    "        return sentence\n",
    "\n",
    "    batch = [trim(ex, field.eos_token) for ex in batch]  # batch를 문장으로 \n",
    "    \n",
    "    def filter_special(tok):\n",
    "        return tok not in (field.init_token, field.pad_token)\n",
    "\n",
    "    batch = [list(filter(filter_special, ex)) for ex in batch]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language = \"de\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language = \"en\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SRC', '<sos>', '<eos>', '<pad>', 'TRG', '<sos>', '<eos>', '<pad>')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    'SRC', SRC.init_token, SRC.eos_token, SRC.pad_token,\n",
    "    'TRG', TRG.init_token, TRG.eos_token, TRG.pad_token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data)\n",
    "TRG.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchtext\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# get iterator (train, valid, test)\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')  # masking with upper triangle\n",
    "    return torch.from_numpy(subsequent_mask) == 0 # reverse (masking=False, non-masking=True)\n",
    "\n",
    "class Batch:\n",
    "    \n",
    "    \"Object for holding a batch of data with masking during training.\" \n",
    "    def __init__(self, src, trg=None, pad=1):\n",
    "        self.src = src.T\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)  # source mask, <pad>: False, other tokens: True\n",
    "        if trg is not None:\n",
    "            #self.trg = trg[:, :-1]  # target sentence 0 ~ -1\n",
    "            #self.trg_y = trg[:, 1:]  # target sentence 1 ~ end\n",
    "            self.trg = trg.T[:, :-1]  # target sentence 0 ~ -1\n",
    "            self.trg_y = trg.T[:, 1:]  # target sentence 1 ~ end\n",
    "            self.trg_mask = self.make_std_mask(self.trg, pad) # target mask\n",
    "            self.ntokens = (self.trg_y != pad).data.sum() # number of tokens\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2) # <pad>: False, other tokens: True, reshape (batch_size, seq_len) -> (batch_size, 1, seq_len)\n",
    "        tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data)) # not <pad> && non-masking: True, others: False\n",
    "        return tgt_mask\n",
    "    \n",
    "    def run_epoch(data_iter, model, loss_compute):\n",
    "        \"Standard Training and Logging Function\"\n",
    "        start = time.time()\n",
    "        total_tokens = 0\n",
    "        total_loss = 0\n",
    "        tokens = 0\n",
    "        pad_index = SRC.vocab[SRC.pad_token]\n",
    "        \n",
    "        for i, batch_without_mask in enumerate(data_iter):\n",
    "            # mask 적용\n",
    "            batch = Batch(batch_without_mask.src, batch_without_mask.trg, pad_index)\n",
    "            \n",
    "            out = model.forward(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "            loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "            total_loss += loss\n",
    "            total_tokens += batch_ntokens\n",
    "            tokens += batch.ntokens\n",
    "            if i % 50 == 1:\n",
    "                elapsed = time.time() - start\n",
    "                print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" % (i, loss / batch.ntokens, tokens / elapsed))\n",
    "                start = time.time()\n",
    "                tokens = 0\n",
    "            \n",
    "        return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 32]) torch.Size([32, 1, 128]) torch.Size([128, 31]) torch.Size([128, 31, 31])\n",
      "tensor([[   2,    5,   70,  ...,    1,    1,    1],\n",
      "        [   2,    5, 2446,  ...,    1,    1,    1],\n",
      "        [   2,    8,   16,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   2,    5,   49,  ...,    1,    1,    1],\n",
      "        [   2,  227,  191,  ...,    1,    1,    1],\n",
      "        [   2,   43,   45,  ...,    1,    1,    1]], device='cuda:0') tensor([[[ True,  True,  True,  ...,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  ...,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  ...,  True,  True,  True]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False]]], device='cuda:0') tensor([[   2,    4, 7382,  ...,    1,    1,    1],\n",
      "        [   2,    4,   41,  ...,    1,    1,    1],\n",
      "        [   2,    4,   14,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   2,    4,   55,  ...,    1,    1,    1],\n",
      "        [   2,    4,   59,  ...,    1,    1,    1],\n",
      "        [   2,   48,   50,  ...,    1,    1,    1]], device='cuda:0') tensor([[[ True, False, False,  ..., False, False, False],\n",
      "         [ True,  True, False,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         ...,\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False]],\n",
      "\n",
      "        [[ True, False, False,  ..., False, False, False],\n",
      "         [ True,  True, False,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         ...,\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False]],\n",
      "\n",
      "        [[ True, False, False,  ..., False, False, False],\n",
      "         [ True,  True, False,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         ...,\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ True, False, False,  ..., False, False, False],\n",
      "         [ True,  True, False,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         ...,\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False]],\n",
      "\n",
      "        [[ True, False, False,  ..., False, False, False],\n",
      "         [ True,  True, False,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         ...,\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False]],\n",
      "\n",
      "        [[ True, False, False,  ..., False, False, False],\n",
      "         [ True,  True, False,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         ...,\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False]]], device='cuda:0')\n",
      "[['a', 'frightened', 'little', 'boy', 'being', 'pulled', 'up', 'and', 'out', 'of', 'a', 'pool', 'by', 'his', 'father', '.'], ['a', 'walking', 'guard', 'stands', 'in', 'front', 'of', 'an', 'asian', 'looking', 'building', '.'], ['a', 'woman', 'in', 'a', 'black', 'shirt', 'looking', 'at', 'a', 'bicycle', '.'], ['a', 'guy', 'is', 'holding', 'a', 'red', 'and', 'white', 'balloon', 'at', 'an', 'indoor', 'party', '.'], ['a', 'soccer', 'player', 'leaps', 'sideways', 'in', 'the', 'air', 'to', 'kick', 'the', 'ball', '.'], ['a', 'man', 'in', 'a', 'black', 't', '-', 'shirt', 'looking', 'down', 'as', 'he', 'holds', 'a', 'drink', '.'], ['two', 'boys', 'are', 'skateboarding', 'in', 'front', 'of', 'an', 'office', 'building', '.'], ['man', 'surfing', 'on', 'a', 'wave', 'in', 'the', 'ocean'], ['a', 'group', 'of', 'people', 'are', 'listening', 'to', 'someone', 'talk', '.'], ['a', 'woman', 'walks', 'down', 'a', 'busy', 'street', 'carrying', 'papers', '.'], ['a', 'woman', 'in', 'a', 'black', 'and', 'white', 'checkered', 'shirt', 'is', 'coloring', 'in', 'a', 'coloring', 'book', 'with', 'a', 'little', 'blond', 'girl', '.'], ['a', 'small', 'boy', 'with', 'an', 'orange', 'jacket', 'digs', 'in', 'the', 'sand', 'in', 'next', 'to', 'a', 'flag', '.'], ['cheerleaders', 'are', 'performing', 'on', 'a', 'football', 'field', '.'], ['a', 'person', 'walks', 'with', 'an', 'artistic', 'contraption', 'in', 'front', 'of', 'the', 'ocean', 'blue', 'company', '.'], ['a', 'man', 'that', 'has', 'a', 'mohawk', 'is', 'at', 'a', 'market', 'with', 'a', 'buggy', 'in', 'hand', '.'], ['a', 'male', 'street', 'performer', 'juggles', 'a', 'brass', 'ring', 'using', 'two', 'sticks', ',', 'while', 'a', 'couple', 'and', 'a', 'male', 'passerby', ',', 'look', 'on', '.'], ['three', 'people', 'wearing', 'colorful', 'costumes', 'and', 'masks', 'sitting', 'between', 'two', 'stone', 'statues', '.'], ['fishermen', 'cast', 'nets', 'from', 'dugout', 'canoes', 'into', 'the', 'water', '.'], ['older', 'woman', 'in', 'a', 'light', 'purple', 'shirt', ',', 'sitting', 'in', 'a', 'chair', ',', 'holding', 'a', 'baby', '.'], ['on', 'the', 'street', 'a', 'man', 'sits', 'and', 'plays', 'the', 'drums', 'while', 'a', 'man', 'standing', 'beside', 'him', 'plays', 'the', 'guitar', '.'], ['a', 'man', 'on', 'a', 'yellow', 'surfboard', 'rides', 'an', 'easy', 'wave', '.'], ['two', 'men', 'playing', 'with', 'their', 'black', 'and', 'white', 'dogs', 'on', 'the', 'beach', '.'], ['female', 'with', 'glasses', ',', 'holding', 'a', 'camera', ',', 'shades', 'her', 'eyes', 'with', 'her', 'hands', 'while', 'standing', 'in', 'the', 'foreground', 'of', 'an', 'ocean', 'view', '.'], ['a', 'child', 'plays', 'in', 'a', 'wiggles', 'car', 'while', 'a', 'man', 'watches', '.'], ['deer', 'and', 'turkeys', 'stand', 'on', 'snow', 'covered', 'ground', '.'], ['a', 'woman', 'plays', 'bass', 'and', 'sings', 'with', 'her', 'bandmate', 'with', 'is', 'a', 'man', 'playing', 'guitar', 'and', 'the', 'drummer', 'plays', 'in', 'the', 'background', '.'], ['three', 'girls', 'playing', 'jump', 'rope', 'outside', '.'], ['a', 'couple', 'people', 'walk', 'on', 'a', 'path', 'with', 'a', 'town', 'in', 'the', 'distance', '.'], ['the', 'eagle', 'is', 'flying', 'low', 'to', 'the', 'ground', '.'], ['a', 'young', 'man', 'smoking', 'at', 'a', 'park', '.'], ['people', 'gathering', 'at', 'the', 'front', 'of', 'the', 'temple', 'before', 'they', 'go', 'to', 'meditate', '.'], ['a', 'man', 'pulls', 'on', 'a', 'rope', 'in', 'the', 'country', '.'], ['a', 'person', 'with', 'an', 'electric', 'drill', 'doing', 'something', 'to', 'a', 'toy', 'animal', '.'], ['a', 'little', 'girl', 'in', 'a', 'pink', 'sweater', 'is', 'using', 'a', 'telescope', 'at', 'night', '.'], ['a', 'child', 'with', 'a', 'stick', 'and', 'no', 'shirt', 'sitting', 'in', 'the', 'grass'], ['there', 'are', 'several', 'people', 'either', 'sitting', 'on', 'benches', 'or', 'walking', 'through', 'what', 'appears', 'to', 'be', 'a', 'brick', 'road', 'with', 'trees', 'coming', 'up', 'from', 'various', 'circular', 'openings', 'in', 'the', 'road', '.'], ['a', 'street', 'fruit', 'vendor', 'waiting', 'for', 'customers', '.'], ['man', 'with', 'an', 'orange', 'shirt', 'is', 'grinding', 'on', 'stone', ',', 'he', 'is', 'not', 'wearing', 'shoes', '.'], ['a', 'woman', 'happily', 'opened', 'her', 'gift', 'and', 'discovers', 'a', 'kitchenaid', 'blender', '.'], ['several', 'women', 'sit', 'holding', 'handrails', 'on', 'an', 'incline', 'shuttle', '.'], ['woman', 'cleans', 'windows', 'in', 'front', 'of', 'store', '.'], ['three', 'scouts', 'are', 'on', 'top', 'of', 'a', 'cliff', 'overlooking', 'the', 'ocean', '.'], ['a', 'couple', 'cutting', 'the', 'cake', 'with', 'an', 'audience', '.'], ['a', 'young', 'boy', 'outside', 'by', 'the', 'pool', ',', 'smiling', '.'], ['a', 'man', 'is', 'holding', 'a', 'large', 'object', ',', 'bending', 'in', 'front', 'of', 'many', 'plants', '.'], ['the', 'brown', 'dog', 'is', 'running', 'through', 'the', 'snow'], ['two', 'men', 'play', 'didgeridoos', 'in', 'front', 'of', 'a', 'fireplace', 'while', 'a', 'siberian', 'husky', 'lays', 'on', 'a', 'couch', '.'], ['two', 'tall', 'black', 'dogs', 'run', 'through', 'tall', 'grass', '.'], ['a', 'man', 'about', 'to', 'step', 'off', 'the', 'edge', 'of', 'a', 'desert', 'rock', 'formation', '.'], ['a', 'woman', 'sings', 'to', 'a', 'group', 'of', 'people', 'holding', 'their', 'hands', 'up', '.'], ['the', 'little', 'boy', 'is', 'about', 'to', 'walk', 'into', 'the', 'sprinklers', '.'], ['a', 'car', 'and', 'many', 'people', 'are', 'standing', 'on', 'the', 'road', '.'], ['a', 'child', 'with', 'a', 'rubber', 'noodle', 'is', 'coached', 'by', 'two', 'men', 'wearing', 'rubber', 'clown', 'noses', '.'], ['a', 'man', 'with', 'black', 'gloves', 'sits', 'on', 'someone', \"'s\", 'shoulders', 'to', 'take', 'a', 'picture', '.'], ['a', 'woman', 'is', 'black', 'is', 'pole', 'vaulting', '.'], ['people', 'in', 'the', 'kitchen', 'at', 'a', 'party', 'with', 'food', 'and', 'drink', '.'], ['a', 'man', 'in', 'shorts', 'and', 'a', 'hawaiian', 'shirt', 'leans', 'over', 'the', 'rail', 'of', 'a', 'pilot', 'boat', ',', 'with', 'fog', 'and', 'mountains', 'in', 'the', 'background', '.'], ['a', 'gray', '-', 'haired', 'man', 'is', 'standing', 'in', 'the', 'water', 'with', 'a', 'yellow', 'bucket', '.'], ['a', 'dog', 'on', 'a', 'leash', 'puts', 'his', 'front', 'paws', 'on', 'a', 'bar', '.'], ['a', 'woman', 'and', 'a', 'young', 'man', 'laughing', 'in', 'a', 'bedroom', '.'], ['two', 'young', 'girls', 'sitting', 'on', 'a', 'sidewalk', 'coloring', 'on', 'it', 'with', 'chalk'], ['people', 'riding', 'in', 'a', 'cyclothon', 'in', 'biking', 'suits', 'and', 'professional', 'helmets', '.'], ['the', 'young', 'man', 'kicks', 'a', 'soccer', 'ball', 'on', 'dusty', 'ground', '.'], ['men', 'are', 'standing', 'around', 'a', 'small', 'train', 'outside', '.'], ['two', 'men', 'are', 'conversing', 'together', 'dressed', 'in', 'traditional', 'clothing', '.'], ['small', 'boy', 'in', 'black', 'shirt', 'laying', 'across', 'bricks', 'and', 'sticking', 'hand', 'in', 'the', 'mud', '.'], ['a', 'man', 'with', 'sunglasses', 'in', 'a', 'pink', 'shirt', '.'], ['three', 'surgeons', 'performing', 'surgery', 'on', 'a', 'patient', '.'], ['the', 'big', 'white', 'cow', 'is', 'decorated', 'with', 'a', 'red', 'ball', 'of', 'yarn', '.'], ['the', 'parachutist', 'has', 'a', 'bird', 'flying', 'along', 'with', 'him', '.'], ['a', 'woman', 'shoots', 'a', 'free', 'throw', 'at', 'a', 'basketball', 'game', '.'], ['a', 'girl', 'is', 'lying', 'on', 'a', 'short', 'brick', 'wall', '.'], ['a', 'well', 'dressed', 'woman', 'is', 'shopping', 'in', 'front', 'of', 'a', 'store', '.'], ['a', 'man', 'in', 'a', 'yellow', 'and', 'yellow', 'shirt', 'puts', 'his', 'arm', 'around', 'a', 'man', 'in', 'a', 'blue', 'shirt', '.'], ['a', 'man', 'at', 'a', 'sporting', 'even', 'dressed', 'in', 'green', 'and', 'yellow', 'is', 'smiling', '.'], ['a', 'large', 'crowd', 'of', 'people', 'sitting', 'in', 'the', 'stands', 'watching', 'a', 'soccer', 'game', '.'], ['the', 'security', 'guard', 'is', 'smiling', '.'], ['children', 'and', 'their', 'parents', 'climbing', 'a', 'metal', 'tower', '.'], ['a', 'man', 'in', 'an', 'elf', 'hat', 'holding', 'a', 'white', 'umbrella', 'is', 'standing', 'on', 'the', 'sidewalk', 'with', 'two', 'other', 'men', '.'], ['a', 'man', 'in', 'a', 'red', 'sweatshirt', 'pushes', 'a', 'giant', 'redwood', 'tree', 'in', 'a', 'snowy', 'forest', '.'], ['older', 'man', 'putting', 'on', 'socks', 'in', 'a', 'treeless', 'sand', 'dune', '.'], ['many', 'people', 'gather', 'to', 'watch', 'a', 'new', 'invention', '.'], ['a', 'man', 'wearing', 'a', 'green', 'headband', 'is', 'working', 'on', 'a', 'construction', 'project', '.'], ['a', 'man', 'with', 'white', 'hair', 'walking', 'down', 'the', 'sidewalk', '.'], ['a', 'policeman', 'is', 'roping', 'off', 'an', 'area', 'with', 'police', 'tape', 'while', 'onlookers', 'watch', '.'], ['a', 'boy', 'recoils', 'from', 'the', 'impact', 'of', 'sand', 'onto', 'his', 'bare', 'chest', '.'], ['a', 'woman', 'wearing', 'red', 'leggings', 'and', 'a', 'white', 'shirt', 'hangs', 'upside', 'down', 'from', 'a', 'metal', 'structure', '.'], ['a', 'clown', 'in', 'a', 'red', 'pointed', 'hat', 'creating', 'balloon', 'art', '.'], ['a', 'man', 'playing', 'a', 'synthesizer', 'wearing', 'a', 'green', 'sweater', 'and', 'hat', '.'], ['two', 'dogs', 'run', 'two', 'a', 'pond', 'on', 'a', 'winter', 'day', '.'], ['construction', 'man', 'paving', 'large', 'concrete', 'steps', '.'], ['three', 'children', 'are', 'playing', 'on', 'the', 'shoreline', 'with', 'a', 'ferry', 'docked', 'in', 'the', 'background', '.'], ['a', 'group', 'of', 'people', ',', 'big', 'and', 'small', ',', 'in', 'a', 'public', 'place', '.'], ['a', 'football', 'player', 'prepares', 'to', 'toss', 'the', 'football', 'as', 'the', 'rest', 'of', 'his', 'team', 'watches', '.'], ['a', 'woman', 'is', 'walking', 'on', 'the', 'sidewalk', 'beside', 'graffiti', 'tagged', 'walls', '.'], ['a', 'male', 'bicyclist', 'trying', 'to', 'get', 'something', 'from', 'his', 'backpack', 'on', 'a', 'busy', 'street', '.'], ['a', 'skateboarder', 'doing', 'a', 'trick', 'in', 'the', 'air', 'on', 'a', 'skating', 'ramp', '.'], ['two', 'women', 'stand', 'near', 'a', 'wall', 'plastered', 'with', 'advertisements', '.'], ['several', 'people', 'are', 'dancing', 'together', 'in', 'sync', '.'], ['blond', 'man', 'with', 'white', 'button', 'up', 'shirt', 'in', 'front', 'of', 'two', 'red', '-', 'haired', 'women', ',', 'one', 'is', 'using', 'a', 'microphone', 'the', 'other', 'wearing', 'headphones', '.'], ['two', 'men', 'acting', 'out', 'a', 'scene', 'in', 'front', 'of', 'a', 'small', 'group', 'of', 'friends', '.'], ['five', 'boys', 'in', 'blue', 'shirts', 'drawing', 'at', 'a', 'table', '.'], ['two', 'policemen', 'are', 'riding', 'bikes', 'on', 'the', 'street', '.'], ['a', 'man', 'with', 'brown', 'hair', 'playing', 'the', 'drums', '.'], ['man', 'rides', 'on', 'bicycle', 'as', 'other', 'people', 'walk', '.'], ['people', 'are', 'skating', 'in', 'a', 'outdoor', 'rink', 'around', 'a', 'huge', 'christmas', 'tree', '.'], ['several', 'women', 'running', 'in', 'a', 'race', ',', 'while', 'the', 'crowd', 'looks', 'on', '.'], ['two', 'donkeys', 'pulling', 'a', 'cart', ',', 'carrying', 'green', 'bushes', ',', 'as', 'well', 'as', 'carrying', 'people', 'on', 'it', '.'], ['a', 'snowboarder', 'doing', 'a', 'trick', 'while', 'on', 'a', 'run', '.'], ['a', 'small', 'boy', 'is', 'sitting', 'on', 'a', 'bench', 'next', 'to', 'a', 'large', 'jelly', 'bean', 'mascot', '.'], ['several', 'people', 'are', 'standing', 'on', 'a', 'sidewalk', 'near', 'a', 'man', 'leaning', 'against', 'a', 'pole', '.'], ['a', 'man', 'in', 'a', 'coca', 'cola', 'building', 'wiping', 'down', 'a', 'table', '.'], ['a', 'couple', 'gazes', 'in', 'awe', 'at', 'a', 'display', 'at', 'a', 'carnival'], ['two', 'men', 'are', 'wrestling', 'while', 'other', 'people', 'are', 'watching'], ['a', 'little', 'boy', 'in', 'a', 'red', 'shirt', 'holding', 'a', 'superman', 'produce', 'and', 'staring', 'at', 'an', 'older', 'man', '.'], ['a', 'man', 'in', 'a', 'wetsuit', 'is', 'surfing', 'on', 'blue', 'water', '.'], ['two', 'senior', 'citizens', 'are', 'watching', 'as', 'a', 'small', 'boy', 'frolics', 'and', 'plays', '.'], ['a', 'black', 'dog', 'running', 'toward', 'the', 'camera', '.'], ['several', 'people', 'laying', 'down', 'on', 'a', 'rocky', 'beach', 'in', 'the', 'foreground', 'with', 'people', 'standing', 'up', 'in', 'the', 'background', '.'], ['a', 'group', 'of', 'women', 'are', 'having', 'makeup', 'applied', '.'], ['a', 'man', 'in', 'construction', 'takes', 'a', 'break', 'while', 'he', 'looks', 'at', 'the', 'view', '.'], ['a', 'man', 'is', 'driving', 'a', 'red', 'atv', 'up', 'a', 'rugged', 'hill', '.'], ['a', 'family', 'of', 'three', 'people', 'are', 'watching', 'men', 'in', 'costumes', 'pass', 'by', '.'], ['several', 'preschool', 'children', 'stand', 'looking', 'through', 'fence', 'bars', 'as', 'two', 'adults', 'look', 'on', '.'], ['a', 'child', 'dressed', 'like', 'a', 'pirate', 'smiles', '.'], ['a', 'child', 'standing', 'in', 'front', 'of', 'a', 'giant', 'mechanical', 'elephant', '.'], ['a', 'large', 'amount', 'of', 'people', 'sit', 'outside', 'some', 'sort', 'of', 'water', 'related', 'event', 'with', 'canadian', 'flags', '.'], ['three', 'women', 'and', 'men', 'selling', 'their', 'items', 'in', 'a', 'cold', 'winter', 'day', '.']]\n",
      "ein kleiner junge , der sich fürchtet und von seinem vater aus einem schwimmbecken gezogen wird .\n",
      "\n",
      "a frightened little boy being pulled up and out of a pool by his father .\n",
      "\n",
      "a walking guard stands in front of an asian looking building .\n",
      "\n",
      "a frightened little boy being pulled up and out of a pool by his father .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "\n",
    "pad_index = SRC.vocab[SRC.pad_token]\n",
    "\n",
    "for batch_without_mask in train_iterator:\n",
    "    batch = Batch(batch_without_mask.src, batch_without_mask.trg, pad_index)\n",
    "    print(batch.src.shape, batch.src_mask.shape, batch.trg.shape, batch.trg_mask.shape)\n",
    "    print(batch.src, batch.src_mask, batch.trg, batch.trg_mask)\n",
    "    print(itos(TRG, batch.trg))\n",
    "    \n",
    "    print(' '.join(itos(SRC, batch.src)[0]))\n",
    "    print()\n",
    "    \n",
    "    print(' '.join(itos(TRG, batch.trg)[0]))\n",
    "    print()\n",
    "    \n",
    "    print(' '.join(itos(TRG, batch.trg)[1]))\n",
    "    print()\n",
    "    \n",
    "    print(' '.join(itos(TRG, batch.trg_y)[0]))\n",
    "    print()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, src_vocab, trg_vocab, seq_len=32, d=512, num_layers=6, h=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "        self.seq_len = 32\n",
    "        self.d = 512\n",
    "        self.encoder = Encoder(num_layers, seq_len, d, h)\n",
    "        self.deccoder = Decoder(num_layers, seq_len, d, h)\n",
    "        \n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        return self.decode(trg, trg_mask, self.encode(src, src_mask), src_mask) # Decoder's src: Encoder's output\n",
    "    \n",
    "    def encode(self, x, mask):\n",
    "        return self.encoder(x, mask)\n",
    "    \n",
    "    def decode(self, x, mask, encoder_output):\n",
    "        return self.decoder(x, mask, encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_layers, seq_len, d, h):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = []\n",
    "        self.seq_len = seq_len\n",
    "        self.d = d\n",
    "        self.h = h\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(EncoderLayer(seq_len, d, h))\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        out = x\n",
    "        for layer in self.layers: \n",
    "            out = self.layers[i](out, mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_layers, seq_len, d, h):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = []\n",
    "        self.seq_len = seq_len\n",
    "        self.d = d\n",
    "        self.h = h\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(DecoderLayer(seq_len, d, h))\n",
    "    \n",
    "    def forward(self, x, mask, encoder_output):\n",
    "        out = x\n",
    "        for layer in self.layers: \n",
    "            out = self.layers[i](x, mask, encoder_output)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_len, d, h):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.d = d\n",
    "        self.h = h\n",
    "        self.multi_head_attention_layer = MultiHeadAttentionLayer(seq_len, d, h)\n",
    "        self.norm_layer = NormLayer(seq_len)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        out = self.multi_head_attention_layer(x, mask)\n",
    "        out = self.norm_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_len, h):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.d = d\n",
    "        self.h = h\n",
    "        self.masked_multi_head_attention_layer = MultiHeadAttentionLayer(seq_len, d, h)\n",
    "        self.multi_head_attention_layer = MultiHeadAttentionLayer(seq_len, d, h)\n",
    "        self.norm_layer = NormLayer(seq_len)\n",
    "    \n",
    "    def forward(self, x, mask, encoder_output):\n",
    "        out = self.masked_multi_head_attention_layer(trg, trg, trg, mask)\n",
    "        out = self.multi_head_attention_layer(trg, encoder_output, encoder_output, mask)\n",
    "        out = self.norm_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_len, d, h):\n",
    "        super(MultiHeadAttentionLayer, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.d = d\n",
    "        self.h = h\n",
    "        self.query_fc_layer = nn.Linear(in_features=d, out_features=d)\n",
    "        self.key_fc_layer = nn.Linear(in_features=d, out_features=d)\n",
    "        self.value_fc_layer = nn.Linear(in_features=d, out_features=d)\n",
    "        self.fc_layer = nn.Linear(in_features=d, out_features=d)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query, key, value's shape: (n_batch, seq_len, d)\n",
    "        \n",
    "        # reshape (n_batch, seq_len, d) to (n_batch, h, seq_len, d_k)\n",
    "        def transform(x, fc_layer):\n",
    "            # x's shape: (n_batch, seq_len, d)\n",
    "            n_batch = x.shape[0]\n",
    "            out = fc_layer(x) # out's shape: (n_batch, seq_len, d)\n",
    "            out = out.view(n_batch, self.seq_len, self.h, self.d//self.h) # out's shape: (n_batch, seq_len, h, d_k)\n",
    "            out = out.transpose(1, 2) # out's shape: (n_batch, h, seq_len, d_k)\n",
    "            return out\n",
    "        \n",
    "        query = transform(query, self.query_fc_layer)      # out's shape: (n_batch, h, seq_len, d_k)\n",
    "        key = transform(key, self.key_fc_layer)\n",
    "        value = transform(value, self.value_fc_layer)\n",
    "        out = calculate_attention(query, key, value, mask) # out's shape: (n_batch, h, seq_len, d_k)\n",
    "        out = out.transpose(1, 2)  # out's shape: (n_batch, seq_len, h, d_k)\n",
    "        out = out.view(n_batch, seq_len, d)  # out's shape: (n_batch, seq_len, d)\n",
    "        out = self.fc_layer(out)  # out's shape: (n_batch, seq_len, d)\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_attention(query, key, value, mask): \n",
    "        d_k = query.size(-1) # key's dimension, query, key, and value's dimensions are same value\n",
    "        score = torch.matmul(query, key.transpose(-2, -1)) # Q x K^T\n",
    "        score = score / math.sqrt(d_k)  # scaling\n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask==0, -1e9)  # masking (Decoder's Masked Multi-Attention Layer)\n",
    "        out = F.softmax(scores, dim = -1) # get softmax score\n",
    "        out = torch.matmul(out, value) # score x V\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. residual connection\n",
    "2. norm layer\n",
    "3. embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderOrDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, kind, num_layers, seq_len, d, h):\n",
    "        super(EncoderOrDecoder, self).__init__()\n",
    "        self.kind = kind\n",
    "        self.layers = []\n",
    "        self.seq_len = seq_len\n",
    "        self.d = d\n",
    "        self.h = h\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(EncoderOrDecoderLayer(kind, seq_len, d, h))\n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "    \n",
    "    def forward(self, x, src_mask, encoder_output, trg_mask):\n",
    "        out = x\n",
    "        for layer in self.layers: \n",
    "            out = self.layers[i](out, src_mask, encoder_output, trg_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderOrDecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, kind, seq_len, d, h):\n",
    "        super(EncoderOrDecodeLayer, self).__init__()\n",
    "        self.kind = kind\n",
    "        self.seq_len = seq_len\n",
    "        self.d = d\n",
    "        self.h = h\n",
    "        if kind == 'Encoder':\n",
    "            self.multi_head_attention_layer = MultiHeadAttentionLayer(seq_len, d, h)\n",
    "        elif kind == 'Decoder':\n",
    "            self.masked_multi_head_attention_layer = MultiHeadAttentionLayer(seq_len, d, h)\n",
    "            self.multi_head_attention_layer = MultiHeadAttentionLayer(seq_len, d, h)\n",
    "        self.norm_layer = NormLayer(seq_len)\n",
    "    \n",
    "    def forward(self, src, src_mask):\n",
    "        if kind == 'Encoder':\n",
    "            out = self.multi_head_attention_layer(src, src_mask)\n",
    "        elif kind == 'Decoder':\n",
    "            out = self.masked_multi_head_attention_layer(trg, trg_mask)\n",
    "            out = self.multi_head_attention_layer(src, src_mask, trg, trg_mask)\n",
    "        out = self.norm_layer(out)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
