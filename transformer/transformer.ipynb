{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator, interleave_keys\n",
    "\n",
    "def itos(field, batch):  # batch에서 원본 sentence 얻는 함수\n",
    "    with torch.cuda.device_of(batch):\n",
    "        #batch = batch.T.tolist()\n",
    "        batch = batch.tolist()\n",
    "    batch = [[field.vocab.itos[ind] for ind in ex] for ex in batch]  # denumericalize\n",
    "    \n",
    "    def trim(s, t):  # 현재 token ~ <EOS> token 사이의 문장 return\n",
    "        sentence = []\n",
    "        for w in s:\n",
    "            if w == t:\n",
    "                break\n",
    "            sentence.append(w)\n",
    "        return sentence\n",
    "\n",
    "    batch = [trim(ex, field.eos_token) for ex in batch]  # batch를 문장으로 \n",
    "    \n",
    "    def filter_special(tok):\n",
    "        return tok not in (field.init_token, field.pad_token)\n",
    "\n",
    "    batch = [list(filter(filter_special, ex)) for ex in batch]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language = \"de\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language = \"en\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data)\n",
    "TRG.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./transformer.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, src_vocab, trg_vocab, src_embed, trg_embed, encoder, decoder, fc_layer):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "        self.src_embed = src_embed\n",
    "        self.trg_embed = trg_embed\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.fc_layer = fc_layer\n",
    "        \n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        out = self.encode(src, src_mask)\n",
    "        out = self.decode(trg, trg_mask, out, src_mask) # Decoder's src: Encoder's output\n",
    "        out = self.fc_layer(out)\n",
    "        #out = F.Logsoftmax(out, dim=-1)\n",
    "        return out\n",
    "    \n",
    "    def encode(self, x, mask):\n",
    "        out = self.encoder(self.src_embed(x), mask)\n",
    "        return out\n",
    "    \n",
    "    def decode(self, x, mask, encoder_output, encoder_mask):\n",
    "        out = self.decoder(self.trg_embed(x), mask, encoder_output, encoder_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, sub_layer, n_layer):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = []\n",
    "        for i in range(n_layer):\n",
    "            self.layers.append(copy.deepcopy(sub_layer))\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        out = x\n",
    "        for layer in self.layers: \n",
    "            out = layer(out, mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, sub_layer, n_layer):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = []\n",
    "        for i in range(n_layer):\n",
    "            self.layers.append(copy.deepcopy(sub_layer))\n",
    "    \n",
    "    def forward(self, x, mask, encoder_output, encoder_mask):\n",
    "        out = x\n",
    "        for layer in self.layers: \n",
    "            out = layer(x, mask, encoder_output, encoder_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, multi_head_attention_layer, fc_layer, residual_connection=True):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        if residual_connection:\n",
    "            self.multi_head_attention_layer = ResidualConnectionLayer(multi_head_attention_layer)\n",
    "            self.fc_layer = ResidualConnectionLayer(fc_layer)\n",
    "        else:\n",
    "            self.multi_head_attention_layer = multi_head_attention_layer\n",
    "            self.fc_layer = fc_layer\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        out = self.multi_head_attention_layer(query=x, key=x, value=x, mask=mask)\n",
    "        out = self.fc_layer(x=out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, multi_head_attention_layer, masked_multi_head_attention_layer, fc_layer, residual_connection=True):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        if residual_connection:\n",
    "            self.masked_multi_head_attention_layer = ResidualConnectionLayer(multi_head_attention_layer)\n",
    "            self.multi_head_attention_layer = ResidualConnectionLayer(masked_multi_head_attention_layer)\n",
    "            self.fc_layer = ResidualConnectionLayer(fc_layer)\n",
    "        else:\n",
    "            self.masked_multi_head_attention_layer = multi_head_attention_layer\n",
    "            self.multi_head_attention_layer = masked_multi_head_attention_layer\n",
    "            self.fc_layer = fc_layer\n",
    "    \n",
    "    def forward(self, x, mask, encoder_output, encoder_mask):\n",
    "        out = self.masked_multi_head_attention_layer(query=x, key=x, value=x, mask=mask)\n",
    "        out = self.multi_head_attention_layer(query=x, key=encoder_output, value=encoder_output, mask=encoder_mask)\n",
    "        out = self.fc_layer(x=out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./self_attention.jpg\" width=\"200\"> <img src=\"./multi_head_attention.jpg\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./attention.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_embed, d_model, n_head, qkv_fc_layer, fc_layer):\n",
    "        super(MultiHeadAttentionLayer, self).__init__()\n",
    "        self.d_embed = d_embed\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.query_fc_layer = copy.deepcopy(qkv_fc_layer)\n",
    "        self.key_fc_layer = copy.deepcopy(qkv_fc_layer)\n",
    "        self.value_fc_layer = copy.deepcopy(qkv_fc_layer)\n",
    "        self.fc_layer = copy.deepcopy(fc_layer)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query, key, value's shape: (n_batch, seq_len, d_embed)\n",
    "        n_batch = query.shape[0]\n",
    "        \n",
    "        # reshape (n_batch, seq_len, d_embed) to (n_batch, n_head, seq_len, d_k)\n",
    "        def transform(x, fc_layer):\n",
    "            # x's shape: (n_batch, seq_len, d_embed)\n",
    "            out = fc_layer(x) # d_embed -> d_model, out's shape: (n_batch, seq_len, d_model)\n",
    "            out = out.view(n_batch, -1, self.n_head, self.d_model//self.n_head) # out's shape: (n_batch, seq_len, n_head, d_k) notice: d_k == d_model//n_head\n",
    "            out = out.transpose(1, 2) # out's shape: (n_batch, n_head, seq_len, d_k)\n",
    "            return out\n",
    "        \n",
    "        query = transform(query, self.query_fc_layer)      # query, key, value's shape: (n_batch, n_head, seq_len, d_k)\n",
    "        key = transform(key, self.key_fc_layer)\n",
    "        value = transform(value, self.value_fc_layer)\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            \n",
    "        out = self.calculate_attention(query, key, value, mask) # out's shape: (n_batch, n_head, seq_len, d_k)\n",
    "        out = out.transpose(1, 2)  # out's shape: (n_batch, seq_len, n_head, d_k)\n",
    "        out = out.contiguous().view(n_batch, -1, self.d_model)  # out's shape: (n_batch, seq_len, d_model)\n",
    "        out = self.fc_layer(out)  # d-model -> d_embed, out's shape: (n_batch, seq_len, d_embed)\n",
    "        return out\n",
    "    \n",
    "    def calculate_attention(self, query, key, value, mask): \n",
    "        d_k = key.size(-1) # query, key, value's shape: (n_batch, n_head, seq_len, d_k)\n",
    "        score = torch.matmul(query, key.transpose(-2, -1)) # Q x K^T\n",
    "        score = score / math.sqrt(d_k)  # scaling\n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask==0, -1e9)  # masking (Decoder's Masked Multi-Attention Layer)\n",
    "        out = F.softmax(score, dim = -1) # get softmax score\n",
    "        out = torch.matmul(out, value) # score x V\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnectionLayer(nn.Module):\n",
    "    def __init__(self, sub_layer):\n",
    "        super(ResidualConnectionLayer, self).__init__()\n",
    "        self.sub_layer = sub_layer\n",
    "    \n",
    "    def forward(self, **kwargs):\n",
    "        if 'x' in kwargs.keys():\n",
    "            x = kwargs['x']\n",
    "            out = x + self.sub_layer(x)\n",
    "        elif 'query' in kwargs.keys():\n",
    "            x = kwargs['query']\n",
    "            out = x + self.sub_layer(**kwargs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_embed, max_seq_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        encoding = torch.zeros(max_seq_len, d_embed)\n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_embed, 2) * -(math.log(10000.0) / d_embed))\n",
    "        encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        encoding = encoding.unsqueeze(0)\n",
    "        self.encoding = encoding\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = x + Variable(self.encoding[:, :x.size(1)], requires_grad=False).to(device)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, d_embed, vocab):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), d_embed)\n",
    "        self.vocab = vocab\n",
    "        self.d_embed = d_embed\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x) * math.sqrt(self.d_embed)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, embedding, positional_encoding):\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        self.embedding = nn.Sequential(embedding, positional_encoding)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, trg_vocab, d_embed=512, n_layer=6, d_model=512, n_head=8):\n",
    "    multi_head_attention_layer = MultiHeadAttentionLayer(d_embed = d_embed,\n",
    "                                                         d_model = d_model,\n",
    "                                                         n_head = n_head,\n",
    "                                                         qkv_fc_layer = nn.Linear(d_embed, d_model),\n",
    "                                                         fc_layer = nn.Linear(d_model, d_embed))\n",
    "    model = Transformer(src_vocab = src_vocab, \n",
    "                        trg_vocab = trg_vocab, \n",
    "                        src_embed = TransformerEmbedding(Embedding(d_embed, src_vocab), PositionalEncoding(d_embed)), \n",
    "                        trg_embed = TransformerEmbedding(Embedding(d_embed, trg_vocab), PositionalEncoding(d_embed)),\n",
    "                        encoder = Encoder(sub_layer = EncoderLayer(multi_head_attention_layer = copy.deepcopy(multi_head_attention_layer).to(device),\n",
    "                                                                   fc_layer = nn.Linear(d_embed, d_embed),\n",
    "                                                                   residual_connection = True),\n",
    "                                          n_layer = n_layer),\n",
    "                        decoder = Decoder(sub_layer = DecoderLayer(multi_head_attention_layer = copy.deepcopy(multi_head_attention_layer).to(device),\n",
    "                                                                  masked_multi_head_attention_layer = copy.deepcopy(multi_head_attention_layer).to(device),\n",
    "                                                                  fc_layer = nn.Linear(d_embed, d_embed),\n",
    "                                                                  residual_connection = True),\n",
    "                                          n_layer = n_layer),\n",
    "                        fc_layer = nn.Linear(d_model, len(trg_vocab)).to(device))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset, Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator, interleave_keys\n",
    "\n",
    "def itos(field, batch):  # batch에서 원본 sentence 얻는 함수\n",
    "    with torch.cuda.device_of(batch):\n",
    "        #batch = batch.T.tolist()\n",
    "        batch = batch.tolist()\n",
    "    batch = [[field.vocab.itos[ind] for ind in ex] for ex in batch]  # denumericalize\n",
    "    \n",
    "    def trim(s, t):  # 현재 token ~ <EOS> token 사이의 문장 return\n",
    "        sentence = []\n",
    "        for w in s:\n",
    "            if w == t:\n",
    "                break\n",
    "            sentence.append(w)\n",
    "        return sentence\n",
    "\n",
    "    batch = [trim(ex, field.eos_token) for ex in batch]  # batch를 문장으로 \n",
    "    \n",
    "    def filter_special(tok):\n",
    "        return tok not in (field.init_token, field.pad_token)\n",
    "\n",
    "    batch = [list(filter(filter_special, ex)) for ex in batch]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "# get iterator (train, valid, test)\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')  # masking with upper triangle\n",
    "    return torch.from_numpy(subsequent_mask) == 0 # reverse (masking=False, non-masking=True)\n",
    "\n",
    "class Batch:\n",
    "    \n",
    "    \"Object for holding a batch of data with masking during training.\" \n",
    "    def __init__(self, src, trg=None, pad=1):\n",
    "        self.src = src.T\n",
    "        self.src_mask = (self.src != pad).unsqueeze(-2)  # source mask, <pad>: False, other tokens: True\n",
    "        if trg is not None:\n",
    "            self.trg = trg.T[:, :-1]  # target sentence 0 ~ -1\n",
    "            self.trg_y = trg.T[:, 1:]  # target sentence 1 ~ end\n",
    "            self.trg_mask = self.make_std_mask(self.trg, pad) # target mask\n",
    "            self.ntokens = (self.trg_y != pad).data.sum() # number of tokens\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2) # <pad>: False, other tokens: True, reshape (batch_size, seq_len) -> (batch_size, 1, seq_len)\n",
    "        tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data)) # not <pad> && non-masking: True, others: False\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 25]) torch.Size([128, 1, 25]) torch.Size([128, 25]) torch.Size([128, 25, 25])\n",
      "src[0]:  bauarbeiter balancieren auf einem schmalen balken , hoch über dem boden .\n",
      "\n",
      "trg[0]:  construction workers balance on a narrow beam far above the ground .\n",
      "\n",
      "trg[1]:  one man is visible riding a black horse with a brown horse and bull in the background .\n",
      "\n",
      "trg_y[0]:  construction workers balance on a narrow beam far above the ground .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "\n",
    "pad_index = SRC.vocab[SRC.pad_token]\n",
    "\n",
    "for i, batch_without_mask in enumerate(train_iterator):\n",
    "    batch = Batch(batch_without_mask.src, batch_without_mask.trg, pad_index)\n",
    "    print(batch.src.shape, batch.src_mask.shape, batch.trg.shape, batch.trg_mask.shape)\n",
    "    \n",
    "    print('src[0]: ', ' '.join(itos(SRC, batch.src)[0]))\n",
    "    print()\n",
    "    \n",
    "    print('trg[0]: ', ' '.join(itos(TRG, batch.trg)[0]))\n",
    "    print()\n",
    "    \n",
    "    print('trg[1]: ', ' '.join(itos(TRG, batch.trg)[1]))\n",
    "    print()\n",
    "    \n",
    "    print('trg_y[0]: ', ' '.join(itos(TRG, batch.trg_y)[0]))\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tranining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (src_embed): TransformerEmbedding(\n",
      "    (embedding): Sequential(\n",
      "      (0): Embedding(\n",
      "        (embedding): Embedding(18668, 512)\n",
      "      )\n",
      "      (1): PositionalEncoding()\n",
      "    )\n",
      "  )\n",
      "  (trg_embed): TransformerEmbedding(\n",
      "    (embedding): Sequential(\n",
      "      (0): Embedding(\n",
      "        (embedding): Embedding(9799, 512)\n",
      "      )\n",
      "      (1): PositionalEncoding()\n",
      "    )\n",
      "  )\n",
      "  (encoder): Encoder()\n",
      "  (decoder): Decoder()\n",
      "  (fc_layer): Linear(in_features=512, out_features=9799, bias=True)\n",
      ")\n",
      "19601991\n"
     ]
    }
   ],
   "source": [
    "model = make_model(SRC.vocab, TRG.vocab, d_embed=512, n_layer=6, d_model=512, n_head=8)\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "# get num of parameters\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "n_parameter = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(n_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute, optimizer):\n",
    "    \"Standard Training and Logging Function\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    pad_index = SRC.vocab[SRC.pad_token]\n",
    "\n",
    "    for i, batch_without_mask in enumerate(data_iter):\n",
    "        # mask 적용\n",
    "        batch = Batch(batch_without_mask.src, batch_without_mask.trg, pad_index)\n",
    "\n",
    "        batch.src = batch.src.to(device)\n",
    "        batch.trg = batch.trg.to(device)\n",
    "        batch.src_mask = batch.src_mask.to(device)\n",
    "        batch.trg_mask = batch.trg_mask.to(device)\n",
    "        out = model.forward(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "        out = out.transpose(1, -1) # reshape for CrossEntropyLoss (#batch, #seq, #vocab) -> (#batch, #vocab, #seq)\n",
    "        #print(out.shape)\n",
    "        #print(batch.trg_y.shape)\n",
    "        #loss = loss_compute(out.contiguous().transpose(-2, -1), batch.trg_y.contiguous())\n",
    "        loss = loss_compute(out, batch.trg_y)\n",
    "        #print(loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" % (i, total_loss / total_tokens, tokens / elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "\n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step: 1 Loss: 283.147400 Tokens per Sec: 1851.262207\n",
      "Epoch Step: 51 Loss: 287.860504 Tokens per Sec: 1737.088257\n",
      "Epoch Step: 101 Loss: 280.713104 Tokens per Sec: 1717.550659\n",
      "Epoch Step: 151 Loss: 278.570465 Tokens per Sec: 1779.993408\n",
      "Epoch Step: 201 Loss: 278.388885 Tokens per Sec: 1664.557495\n",
      "Epoch Step: 1 Loss: 217.146210 Tokens per Sec: 1558.296143\n",
      "Epoch Step: 51 Loss: 220.008453 Tokens per Sec: 1697.605225\n",
      "Epoch Step: 101 Loss: 229.331635 Tokens per Sec: 1744.115723\n",
      "Epoch Step: 151 Loss: 235.345016 Tokens per Sec: 1708.278198\n",
      "Epoch Step: 201 Loss: 240.260284 Tokens per Sec: 1733.173828\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-223-c63a20e277f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-221-e7b86e78c85e>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(data_iter, model, loss_compute, optimizer)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#print(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "n_epoch = 5\n",
    "\n",
    "#criterion = nn.NLLLoss()\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    model.train()\n",
    "    run_epoch(train_iterator, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3112)\n",
      "tensor(2.3112)\n",
      "tensor(2.3112)\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[0.8982, 0.805, 0.6393, 0.9983, 0.5731, 0.0469, 0.556, 0.1476, 0.8404, 0.1476, 0.8404, 0.5544]])\n",
    "y = torch.LongTensor([1])\n",
    "\n",
    "cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "print(cross_entropy_loss(x, y))\n",
    "\n",
    "log_softmax = nn.LogSoftmax(dim=1)\n",
    "x_log = log_softmax(x)\n",
    "def NLLLoss(logs, targets):\n",
    "    out = torch.zeros_like(targets, dtype=torch.float)\n",
    "    for i in range(len(targets)):\n",
    "        out[i] = logs[i][targets[i]]\n",
    "    return -out.sum()/len(out)\n",
    "print(NLLLoss(x_log, y))\n",
    "\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "print(nll_loss(x_log, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(data_iter):\n",
    "    model.eval()\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    pad_index = SRC.vocab[SRC.pad_token]\n",
    "\n",
    "    for i, batch_without_mask in enumerate(data_iter):\n",
    "        # mask 적용\n",
    "        batch = Batch(batch_without_mask.src, batch_without_mask.trg, pad_index)\n",
    "\n",
    "        batch.src = batch.src.to(device)\n",
    "        batch.trg = batch.trg.to(device)\n",
    "        batch.src_mask = batch.src_mask.to(device)\n",
    "        batch.trg_mask = batch.trg_mask.to(device)\n",
    "        out = model.forward(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "        loss = loss_compute(out.contiguous().transpose(-2, -1), batch.trg_y.contiguous())\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" % (i, loss / batch.ntokens, tokens / elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "\n",
    "    print(\"Final: \", total_loss / total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1793, 0.2996, 0.1393, 0.1840, 0.1978],\n",
      "        [0.1153, 0.0821, 0.4615, 0.3128, 0.0282],\n",
      "        [0.0667, 0.1453, 0.4121, 0.3134, 0.0625]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 0, 4])\n",
      "tensor([1, 2, 2], grad_fn=<NotImplemented>)\n",
      "tensor(-0.1591, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "m = nn.Softmax(dim=1)\n",
    "loss = nn.NLLLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "\n",
    "target = torch.tensor([1, 0, 4])\n",
    "softmax = m(input)\n",
    "output = loss(softmax, target)\n",
    "output.backward()\n",
    "\n",
    "print(softmax)\n",
    "print(target)\n",
    "print(torch.argmax(softmax, dim=1))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4, 8, 8])\n",
      "torch.Size([5, 4, 8, 8])\n",
      "tensor([[[[0.1257, 0.3147, 0.1313,  ..., 0.0672, 0.2222, 0.3530],\n",
      "          [0.3009, 0.3475, 0.1587,  ..., 0.4168, 0.2699, 0.1418],\n",
      "          [0.1525, 0.1346, 0.1601,  ..., 0.4446, 0.2420, 0.2255],\n",
      "          ...,\n",
      "          [0.1057, 0.1527, 0.0767,  ..., 0.4231, 0.1867, 0.3147],\n",
      "          [0.2915, 0.3935, 0.4222,  ..., 0.2901, 0.0910, 0.2017],\n",
      "          [0.2523, 0.4173, 0.1394,  ..., 0.1240, 0.1669, 0.2435]],\n",
      "\n",
      "         [[0.3368, 0.3739, 0.3243,  ..., 0.1857, 0.2341, 0.2075],\n",
      "          [0.2632, 0.1699, 0.2498,  ..., 0.1559, 0.2550, 0.3730],\n",
      "          [0.3154, 0.4212, 0.2729,  ..., 0.2056, 0.3012, 0.2725],\n",
      "          ...,\n",
      "          [0.4808, 0.4178, 0.1734,  ..., 0.2439, 0.3152, 0.2723],\n",
      "          [0.1382, 0.2605, 0.2284,  ..., 0.1918, 0.3403, 0.3044],\n",
      "          [0.2736, 0.1364, 0.4364,  ..., 0.2642, 0.2774, 0.1141]],\n",
      "\n",
      "         [[0.3692, 0.1042, 0.1943,  ..., 0.1924, 0.1289, 0.1675],\n",
      "          [0.2561, 0.2766, 0.2956,  ..., 0.2508, 0.2486, 0.1095],\n",
      "          [0.1579, 0.1435, 0.3315,  ..., 0.1422, 0.1217, 0.2155],\n",
      "          ...,\n",
      "          [0.3043, 0.2331, 0.6250,  ..., 0.2121, 0.3107, 0.1088],\n",
      "          [0.2424, 0.2582, 0.1420,  ..., 0.3315, 0.1530, 0.2266],\n",
      "          [0.2317, 0.2791, 0.1956,  ..., 0.2654, 0.2762, 0.4365]],\n",
      "\n",
      "         [[0.1683, 0.2072, 0.3502,  ..., 0.5547, 0.4148, 0.2719],\n",
      "          [0.1798, 0.2059, 0.2959,  ..., 0.1765, 0.2266, 0.3757],\n",
      "          [0.3743, 0.3007, 0.2355,  ..., 0.2076, 0.3352, 0.2865],\n",
      "          ...,\n",
      "          [0.1092, 0.1964, 0.1248,  ..., 0.1210, 0.1875, 0.3042],\n",
      "          [0.3280, 0.0878, 0.2075,  ..., 0.1866, 0.4158, 0.2673],\n",
      "          [0.2424, 0.1672, 0.2286,  ..., 0.3465, 0.2795, 0.2058]]],\n",
      "\n",
      "\n",
      "        [[[0.2122, 0.3005, 0.5398,  ..., 0.3558, 0.1434, 0.3657],\n",
      "          [0.3485, 0.1223, 0.4881,  ..., 0.4940, 0.2546, 0.1362],\n",
      "          [0.2628, 0.3337, 0.1539,  ..., 0.3352, 0.2193, 0.0890],\n",
      "          ...,\n",
      "          [0.2265, 0.1777, 0.2369,  ..., 0.2372, 0.0533, 0.1704],\n",
      "          [0.3136, 0.2064, 0.3743,  ..., 0.1414, 0.3993, 0.1989],\n",
      "          [0.2851, 0.3523, 0.2804,  ..., 0.1502, 0.1962, 0.3796]],\n",
      "\n",
      "         [[0.3759, 0.2165, 0.1848,  ..., 0.1294, 0.1211, 0.1067],\n",
      "          [0.1775, 0.4649, 0.2797,  ..., 0.2129, 0.1591, 0.3513],\n",
      "          [0.2541, 0.0804, 0.2527,  ..., 0.2154, 0.2355, 0.2124],\n",
      "          ...,\n",
      "          [0.2028, 0.2109, 0.3628,  ..., 0.3304, 0.2466, 0.3616],\n",
      "          [0.1163, 0.1372, 0.1618,  ..., 0.3160, 0.1026, 0.4152],\n",
      "          [0.1413, 0.2266, 0.2403,  ..., 0.2080, 0.1385, 0.1686]],\n",
      "\n",
      "         [[0.1583, 0.2698, 0.1695,  ..., 0.3147, 0.3466, 0.4014],\n",
      "          [0.2667, 0.0967, 0.0477,  ..., 0.1962, 0.3612, 0.4043],\n",
      "          [0.1479, 0.0882, 0.0847,  ..., 0.1627, 0.3330, 0.2677],\n",
      "          ...,\n",
      "          [0.0864, 0.1953, 0.2360,  ..., 0.2627, 0.2181, 0.3014],\n",
      "          [0.2575, 0.4870, 0.3242,  ..., 0.3375, 0.3034, 0.1645],\n",
      "          [0.3445, 0.1408, 0.3617,  ..., 0.3283, 0.5282, 0.3387]],\n",
      "\n",
      "         [[0.2536, 0.2131, 0.1059,  ..., 0.2001, 0.3890, 0.1261],\n",
      "          [0.2073, 0.3161, 0.1845,  ..., 0.0969, 0.2250, 0.1082],\n",
      "          [0.3352, 0.4977, 0.5087,  ..., 0.2867, 0.2122, 0.4309],\n",
      "          ...,\n",
      "          [0.4844, 0.4161, 0.1644,  ..., 0.1697, 0.4821, 0.1665],\n",
      "          [0.3126, 0.1694, 0.1397,  ..., 0.2052, 0.1947, 0.2214],\n",
      "          [0.2291, 0.2803, 0.1176,  ..., 0.3135, 0.1371, 0.1131]]],\n",
      "\n",
      "\n",
      "        [[[0.1067, 0.2522, 0.2611,  ..., 0.2994, 0.1664, 0.1396],\n",
      "          [0.3132, 0.1728, 0.2033,  ..., 0.1645, 0.2419, 0.1353],\n",
      "          [0.2537, 0.5777, 0.2269,  ..., 0.3138, 0.2591, 0.2348],\n",
      "          ...,\n",
      "          [0.1803, 0.2672, 0.3412,  ..., 0.1525, 0.2900, 0.1610],\n",
      "          [0.2229, 0.1770, 0.2756,  ..., 0.1809, 0.2743, 0.1628],\n",
      "          [0.2413, 0.2493, 0.1409,  ..., 0.2356, 0.2325, 0.1581]],\n",
      "\n",
      "         [[0.3960, 0.3504, 0.3532,  ..., 0.2005, 0.3103, 0.4561],\n",
      "          [0.1403, 0.2439, 0.4034,  ..., 0.2199, 0.3662, 0.2488],\n",
      "          [0.1585, 0.2047, 0.2721,  ..., 0.1820, 0.2010, 0.2991],\n",
      "          ...,\n",
      "          [0.3072, 0.1964, 0.1309,  ..., 0.1437, 0.2737, 0.5074],\n",
      "          [0.2819, 0.3527, 0.1655,  ..., 0.2584, 0.2870, 0.3982],\n",
      "          [0.3592, 0.1807, 0.5362,  ..., 0.4490, 0.1294, 0.4376]],\n",
      "\n",
      "         [[0.1509, 0.1963, 0.1451,  ..., 0.3098, 0.1841, 0.2675],\n",
      "          [0.1976, 0.5009, 0.1867,  ..., 0.5191, 0.3196, 0.5216],\n",
      "          [0.4559, 0.1350, 0.1605,  ..., 0.2908, 0.1866, 0.1238],\n",
      "          ...,\n",
      "          [0.1965, 0.2840, 0.2823,  ..., 0.5564, 0.2652, 0.1971],\n",
      "          [0.3940, 0.3641, 0.2163,  ..., 0.3097, 0.2815, 0.1271],\n",
      "          [0.2606, 0.2313, 0.1789,  ..., 0.1041, 0.1425, 0.1942]],\n",
      "\n",
      "         [[0.3463, 0.2011, 0.2406,  ..., 0.1902, 0.3392, 0.1368],\n",
      "          [0.3489, 0.0823, 0.2066,  ..., 0.0966, 0.0723, 0.0944],\n",
      "          [0.1319, 0.0826, 0.3405,  ..., 0.2134, 0.3533, 0.3423],\n",
      "          ...,\n",
      "          [0.3160, 0.2524, 0.2455,  ..., 0.1474, 0.1711, 0.1345],\n",
      "          [0.1012, 0.1062, 0.3425,  ..., 0.2509, 0.1571, 0.3120],\n",
      "          [0.1389, 0.3388, 0.1440,  ..., 0.2113, 0.4956, 0.2100]]],\n",
      "\n",
      "\n",
      "        [[[0.1989, 0.3149, 0.2813,  ..., 0.1523, 0.2906, 0.0784],\n",
      "          [0.3006, 0.1736, 0.1695,  ..., 0.0947, 0.1841, 0.1903],\n",
      "          [0.1481, 0.1619, 0.2141,  ..., 0.0949, 0.3055, 0.2717],\n",
      "          ...,\n",
      "          [0.2099, 0.1473, 0.2758,  ..., 0.3064, 0.4166, 0.3042],\n",
      "          [0.4658, 0.1482, 0.2486,  ..., 0.2777, 0.2449, 0.2918],\n",
      "          [0.4157, 0.2142, 0.0931,  ..., 0.2077, 0.2644, 0.4419]],\n",
      "\n",
      "         [[0.2737, 0.0868, 0.3433,  ..., 0.5918, 0.4104, 0.4885],\n",
      "          [0.2390, 0.1549, 0.5073,  ..., 0.2049, 0.5250, 0.4223],\n",
      "          [0.1362, 0.4350, 0.2846,  ..., 0.0729, 0.1797, 0.2777],\n",
      "          ...,\n",
      "          [0.1093, 0.1506, 0.2209,  ..., 0.1366, 0.2188, 0.3474],\n",
      "          [0.1389, 0.2027, 0.0907,  ..., 0.3262, 0.1520, 0.1688],\n",
      "          [0.1292, 0.3271, 0.1778,  ..., 0.2839, 0.2376, 0.2518]],\n",
      "\n",
      "         [[0.1244, 0.3474, 0.1899,  ..., 0.1946, 0.1554, 0.3517],\n",
      "          [0.2333, 0.4721, 0.1776,  ..., 0.5627, 0.1394, 0.1638],\n",
      "          [0.3127, 0.1353, 0.3164,  ..., 0.7373, 0.2958, 0.2899],\n",
      "          ...,\n",
      "          [0.2094, 0.3904, 0.3586,  ..., 0.1857, 0.2405, 0.0895],\n",
      "          [0.2934, 0.3556, 0.4443,  ..., 0.1166, 0.3157, 0.2665],\n",
      "          [0.1316, 0.2063, 0.0969,  ..., 0.2021, 0.2196, 0.1610]],\n",
      "\n",
      "         [[0.4029, 0.2509, 0.1856,  ..., 0.0613, 0.1436, 0.0814],\n",
      "          [0.2271, 0.1994, 0.1456,  ..., 0.1377, 0.1515, 0.2236],\n",
      "          [0.4029, 0.2677, 0.1850,  ..., 0.0948, 0.2190, 0.1607],\n",
      "          ...,\n",
      "          [0.4714, 0.3117, 0.1446,  ..., 0.3713, 0.1241, 0.2588],\n",
      "          [0.1018, 0.2935, 0.2163,  ..., 0.2795, 0.2874, 0.2729],\n",
      "          [0.3235, 0.2523, 0.6321,  ..., 0.3063, 0.2784, 0.1452]]],\n",
      "\n",
      "\n",
      "        [[[0.5637, 0.2038, 0.1395,  ..., 0.1927, 0.1541, 0.1586],\n",
      "          [0.4202, 0.5340, 0.3250,  ..., 0.1132, 0.3012, 0.0762],\n",
      "          [0.1512, 0.2427, 0.2401,  ..., 0.3458, 0.3625, 0.2628],\n",
      "          ...,\n",
      "          [0.2672, 0.5862, 0.1618,  ..., 0.2245, 0.0960, 0.3667],\n",
      "          [0.1911, 0.2541, 0.5015,  ..., 0.3851, 0.1357, 0.2427],\n",
      "          [0.1990, 0.1395, 0.2514,  ..., 0.5653, 0.2317, 0.3073]],\n",
      "\n",
      "         [[0.1769, 0.1879, 0.2902,  ..., 0.2245, 0.1913, 0.1256],\n",
      "          [0.1491, 0.0597, 0.0400,  ..., 0.2399, 0.1714, 0.2019],\n",
      "          [0.2749, 0.1562, 0.1463,  ..., 0.1105, 0.1559, 0.3984],\n",
      "          ...,\n",
      "          [0.2467, 0.0580, 0.1617,  ..., 0.2857, 0.1425, 0.1730],\n",
      "          [0.2315, 0.1120, 0.0663,  ..., 0.2165, 0.2955, 0.1792],\n",
      "          [0.4268, 0.1853, 0.1473,  ..., 0.1913, 0.3633, 0.1290]],\n",
      "\n",
      "         [[0.1397, 0.4587, 0.1869,  ..., 0.2399, 0.1248, 0.4431],\n",
      "          [0.2012, 0.2029, 0.4426,  ..., 0.3304, 0.2879, 0.4687],\n",
      "          [0.1919, 0.1956, 0.1769,  ..., 0.4227, 0.3937, 0.2420],\n",
      "          ...,\n",
      "          [0.1619, 0.0872, 0.1637,  ..., 0.0746, 0.5386, 0.1671],\n",
      "          [0.3669, 0.3350, 0.3234,  ..., 0.1719, 0.2612, 0.4256],\n",
      "          [0.2809, 0.4383, 0.2146,  ..., 0.0626, 0.1674, 0.2652]],\n",
      "\n",
      "         [[0.1197, 0.1496, 0.3834,  ..., 0.3429, 0.5299, 0.2728],\n",
      "          [0.2295, 0.2034, 0.1923,  ..., 0.3165, 0.2396, 0.2532],\n",
      "          [0.3820, 0.4056, 0.4368,  ..., 0.1210, 0.0879, 0.0967],\n",
      "          ...,\n",
      "          [0.3242, 0.2685, 0.5128,  ..., 0.4152, 0.2230, 0.2932],\n",
      "          [0.2104, 0.2988, 0.1088,  ..., 0.2265, 0.3076, 0.1525],\n",
      "          [0.0933, 0.2369, 0.3867,  ..., 0.1808, 0.2375, 0.2985]]]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "torch.Size([5, 8, 8])\n",
      "tensor(-0.2489, grad_fn=<NllLoss2DBackward>)\n"
     ]
    }
   ],
   "source": [
    "N, C = 5, 4\n",
    "loss = nn.NLLLoss()\n",
    "data = torch.randn(N, 16, 10, 10)\n",
    "conv = nn.Conv2d(16, C, (3,3))\n",
    "m = nn.Softmax(dim=1)\n",
    "c = conv(data)\n",
    "softmax = m(c)\n",
    "target = torch.empty(N, 8, 8, dtype=torch.long).random_(0,C)\n",
    "output = loss(softmax, target)\n",
    "output.backward()\n",
    "\n",
    "print(c.shape)\n",
    "print(softmax.shape)\n",
    "print(softmax)\n",
    "print(target.shape)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
